{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data Analysis: Home Exam (2020-2021)\n",
    "\n",
    "> + **Allocated time:** 10 days\n",
    "> + **Send your final notebook** at [romain.madar@cern.ch](mailto:romain.madar@cern.ch) before **28/09/2020 23:59:59 CEST** \n",
    ">\n",
    "> **Comment 1**: the final mark will be a number between 0 (very bad) and 20 (very good). The evaluation of this exam takes into account the correctness of the answers, but also the clarity of the explanations and the quality of the code. *There is no required knowledge about Parkinson disease or general medecine to answer the questions asked in this exam.*\n",
    ">\n",
    "> **Comment 2**: discussions are encouraged,  with the professor (*via* email or just passing by my office) and between students. However you should make sure to demonstrate that *you understand what is in your notebook*.\n",
    "> \n",
    "> This exam is split int 3 parts: (1) basic data exploration, (2) discriminative power analysis, (3) linear correlation modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of context ...\n",
    "\n",
    "\n",
    "The dataset considered in work contains information about Parkinson disease and voice properties of subjects. The goal of this exam is to perform few analysis steps, from exploration to modelling, on an completely unknown dataset. This also demonstrates the generality of the concepts learned during the lecture. The ultimate goal of such dataset - not studied in this exam - would to be able to predict if a subject is healthy, from its voice property: this is a classification problem. All details about this dataset can be found on this [webpage](https://archive.ics.uci.edu/ml/datasets/parkinsons), and the complete scientific citation is:\n",
    "> *Little, M.A., McSharry, P.E., Roberts, S.J. et al.* **Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection**. BioMed Eng OnLine 6, 23 (2007). https://doi.org/10.1186/1475-925X-6-23\n",
    "\n",
    "An overview of the dataset features is given below:\n",
    "+ **name** - ASCII subject name and recording number     \n",
    "+ **MDVP:Fo(Hz)** - Average vocal fundamental frequency    \n",
    "+ **MDVP:Fhi(Hz)** - Maximum vocal fundamental frequency\n",
    "+ **MDVP:Flo(Hz)** - Minimum vocal fundamental frequency\n",
    "+ **MDVP:Jitter(%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP** - Several measures of variation in fundamental frequency\n",
    "+ **MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, MDVP:APQ, Shimmer:DDA** - Several measures of variation in amplitude\n",
    "+ **NHR, HNR** - Two measures of ratio of noise to tonal components in the voice\n",
    "+ **status** - Health status of the subject, one is Parkinson's and zero is healthy\n",
    "+ **RPDE, D2** - Two nonlinear dynamical complexity measures\n",
    "+ **DFA** - Signal fractal scaling exponent\n",
    "+ **spread1, spread2, PPE** - Three nonlinear measures of fundamental frequency variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the usual packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "\n",
    "The data file can be found in `lecture-python/data` directory and is called `parkinsons.data`. This file in the csv format. Create a pandas dataframe `df` from the csv file, and print the first 5 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Basic data exploration (7pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fraction of healty subjects (1pts)\n",
    "\n",
    "Determine the percentage of healty subjects in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Typical range of every features (2pts)\n",
    "\n",
    "When manipulating data, it is important to know if the numbers are large, small, with a large variation, etc ...\n",
    "This allows, for example, to produced plot with adapted scales and then have a good data visualisation. \n",
    "\n",
    "Compute and print the minimum, the maximum and the mean and the RMS values of every features, but `name` (not a number) and `status` (what we want to predict, called the *target*). Universal numpy functions `ufunc` should be used.\n",
    "\n",
    "> **HINT:** one can get a python list of column names from a dataframe using the command `list(df.columns)` (the object `df.columns` is what we call a *pandas serie*, which is not exactly a python list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store these numbers into a \"double dictionnary\" to later access them *via* `statValue[varName][value]` (where `varName` is the name of the feature and `value` can be `min`, `max`, `mean` or `rms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data visualisation: feature distributions (4pts)\n",
    "\n",
    "Before producing actual histograms, it is important to define the proper *histogram binning*. The binning is a collection of windows, or bins, in which observations are counted. When these windows are too wide, we loose the information on the shape of the distribution (*e.g.* only one bin leads to a simple counting). When these windows are too narrow, counts are low and statistical fluctuations degrade the visualisation. \n",
    "\n",
    "\n",
    "a) Write a function which takes the feature name in argument and returns a constant binning (all windows have the same width), *i.e.* a 1-dim numpy array, statisfying:\n",
    " 1. lower edge of the first bin: minimum value of the feature\n",
    " 2. heigher edge of the last bin: maximum value of the feature\n",
    " 3. the RMS corresponds to 2.5 bins\n",
    " 4. if the 3 constraints above cannot be satisfied, one can add one bin at the end so that the maximum value is containted in the binning.\n",
    "\n",
    "> **HINT:** the \"double dictionnary\" defined before might be useful here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the histogram of each feature, but `name` and `status`, using the binning defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discriminative power (8 pts)\n",
    "\n",
    "Now, we want to understand which feature show the largest difference between healthy and Parkinson subjects. This first step, to get an intuition, is to compare distributions for both healty and Parkinson categories. \n",
    "\n",
    "Produce the histogram of every feature both for for healty and parkinson categories. In order to properly compare distribution shape (and not absolute number of observation in each category), one can use the `density=True` option in `plot.hist()` function.\n",
    "\n",
    "\n",
    "### 2.1 Distribution comparison (2pts)\n",
    "\n",
    "a) Modify the plotting code from the previous question to overlay distribution for healty and for Parkinson data. In order to properly compare distribution shape (and not absolute number of observation in each category), one can use the `density=True` option of the `plot.hist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) From the resulting plot, which variables seem to be the most powerful in discriminating healthy subject from Parkinson ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Quantitative estimation (4pts)\n",
    "\n",
    "a) It is also possible to define quantitative estimators for the separation between two distributions. One possibility is to consider:\n",
    "$$\n",
    "s \\; = \\; \\frac{1}{2} \\; \\sum_{i} \\frac{\\left(H_i - P_i\\right)^2}{H_i + P_i}\n",
    "$$\n",
    "where the sum runs one histogram bins. $H_i$ and $P_i$ are the *normalized* contents of the bin $i$ for healthy (H like Healthy) and parkinson (P like Parkinson) categories of the studied variable. A *Normalized* bin content means that the sum over all bins content of the histogram is equal to unity:\n",
    "$$\n",
    "\\sum_{i} H_i = 1.0 ~~\\text{and}~~ \\sum_{i} P_i = 1.0\n",
    "$$\n",
    "\n",
    "a) 1. Write a function which takes a 1-dim array of data and a binning as arguments, and return an numpy array of normalized bin contents. \n",
    "\n",
    "> **HINT:** The content of each bin of an histogram can be obtained from a numpy array `a` for a binning `bins` using `np.histogram(a, bins)`:\n",
    "> ```python\n",
    "> binContents, binEdges = np.histogram(a, bins)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) 2. Loop over features and compute (and print), for each of them, the separation $s$ as defined above. One has to pay attention to `nan` (Not a Number) values, since there are divisions. Indeed, if both $H_i$ and $P_i$ are null, $s$ invovles a division by $0$. For this aspect, the `np.isfinite()` numpy function can be quite useful.\n",
    "\n",
    "Which one has the largest separation? What its the separation value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Display this information using a bar plot, ranked from the most to the less discriminant variable.\n",
    "\n",
    "> **HINT:** functions `np.sort()` and `np.argsort()` might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Single features are not the whole story (2pts)\n",
    "\n",
    "a) Investigate the correlations between `spread1` and `PPE`. What do you conlcude from this? Check the correlation between `spread1` and the four first higher ranked variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using one pandas plotting function seen in the lecture, check the correlations between the 5 features with the highest separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling of the linear correlation between `spread1` and `PPE` (5pts)\n",
    "\n",
    "We will try to model the linear correlation between `spread1` and `PPE` by the following mathematical function $f(x)$, parametrized by two unknown numbers $(a, b)$:\n",
    "$$\n",
    "y = f(a, b; x) = ax + b\n",
    "$$\n",
    "\n",
    "The goal of what follows it to find the best value of $(a, b)$ which model the data, and try to qualitively assess the quality of the model.\n",
    "\n",
    "### 3.1  Loss function definition (2 pts) \n",
    "\n",
    "The quantity we want to minimize, called *loss function* can be defined as the distance between observed points and predicted points, summed over all observations. Mathematically, it is defined as:\n",
    "$$\n",
    "C(a, b) = \\sqrt{\\sum_i ( y_i - f(a, b; x_i) )^2}\n",
    "$$\n",
    "\n",
    "a) Write a python function which takes in argument the parameter $a$ and $b$ (numbers), as well as $\\{x_i\\}$ and $\\{y_i\\}$ data (two 1-dim numpy array) and returns the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Print the value of the loss function for $(a,b) = (0.1, 1.0)$ when $x$ values are `spread1` and $y$ values are `PPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Plot the loss function for 1000 values of $a$ between $-1$ and $1$, and do this twice, namely for $b=0$ and $b=4$. What do you conclude on the value of $a$ which minimize the loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finding the best model parameters - not optimized version (3 pts)\n",
    "\n",
    "Now, we want to account for the full model parameter space by producing a 2D plot of $C(a,b)$ in order to find which $(a,b)$ value lead the the best linear model.\n",
    "\n",
    "a) First plot $C(a,b)$ for 30 values of $a$ between $-1$ and $1$ and 30 values of $b$ between $-4$ and $-4$. \n",
    "\n",
    "\n",
    "\n",
    "> **HINT:** a 2D array can be plotted *verus* `x` and `y` using `plot.imshow()` with the following options:\n",
    "> ```python\n",
    "> plt.imshow(array2D, interpolation='none', aspect='auto', origin='lower',\n",
    "           extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Find the $(a,b)$ for which $C(a, b)$ is minimum. \n",
    "\n",
    "> **HINT:** the function `np.argmin(a)` will be useful here, but returns the index of the minimum in the *flatten array* (try it on a example of 2-dim array). The numpy function `np.unravel_index(indices)` allows to convert this indices into a n-dim index, which can be used in the originl n-dim array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Plot the best model overlaid with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 BONUS: finding the best model parameters - optimized version (3 pts)\n",
    "\n",
    "This result might not be so nice. One would need more point to scan the parameter space. However, doing loops - without surprise - is quite expensive (try to replace 30x30 by a 1000x1000 grid, for instance). \n",
    "\n",
    "\n",
    "a) Modify the python loss function so that `a` and `b` can be 1-dim arrays (potentially of different size), and not single values, using advanced broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Repeat the parameter scan using 1 millions points, *i.e.* on a $1000 \\times 1000$ grid. Plot the obtained best model, overlaid with data and previously found model (with a $30 \\times 30$ grid). What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 BONUS: comparison of several loss functions and grid granularity (2pts)\n",
    "\n",
    "Re-factorize the code to have a single function taking a loss function and number of grid points as argument, and returning the best parameters. Use this function to compare the results of various loss functions, and the impact of the grid granularity on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
